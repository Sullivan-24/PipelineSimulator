
G = 1024 * 1024 * 1024
M = 1024 * 1024
K = 1024
B = G
DEVICE_NUM = 4 * 2
# 自定义模型
VOCAB_SIZE = 92544
NUM_ATTENTION_HEAD = 32
SEQ_LEN = 4 * K
HIDDEN_SIZE = 4 * K
LAYER_NUM = 4 * 4 * 1
MICRO_BATCH_SIZE = 1
MICRO_BATCH_NUM = 4 * 4 * 1

WORLD_SIZE = DEVICE_NUM
PP_SIZE = DEVICE_NUM
TP_SIZE = 1
MODEL_TYPE = "GPT"

# Qwen 72B I1F1B --------------
VOCAB_SIZE = 152064
HIDDEN_SIZE = 8 * K
SEQ_LEN = 4 * K
LAYER_NUM = 80
NUM_ATTENTION_HEAD=64
PP_SIZE = DEVICE_NUM
TP_SIZE = 8
ZERO_SIZE = 1
# Qwen 72B I1F1B --------------

# Llama2 56B --------------
MODEL_TYPE = "LLAMA"
DEVICE_NUM = 4
VOCAB_SIZE = 128256
MLP_RATIO = 2.6875
HIDDEN_SIZE = 8 * K
INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
SEQ_LEN = 4 * K
LAYER_NUM = 80
NUM_ATTENTION_HEAD=64
PP_SIZE = DEVICE_NUM
TP_SIZE = 4
ZERO_SIZE = 4
MICRO_BATCH_NUM = PP_SIZE * 2
# Llama 56B --------------

# # Qwen 72B --------------
# MODEL_TYPE = "Qwen"
# DEVICE_NUM = 4
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 8 * K
# MLP_RATIO = 49152 / HIDDEN_SIZE
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 80
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 4
# MICRO_BATCH_NUM = PP_SIZE * 4
# # Qwen 72B --------------

# # #Qwen 32B I1F1B --------------
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 5 * K
# SEQ_LEN = 4 * K
# LAYER_NUM = 64
# NUM_ATTENTION_HEAD=40
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# # #Qwen 32B I1F1B --------------

# # #Qwen 14B I1F1B --------------
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 5 * K
# SEQ_LEN = 4 * K
# LAYER_NUM = 48
# NUM_ATTENTION_HEAD=40
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# # #Qwen 14B I1F1B --------------

# # #Qwen 7(8)B I1F1B --------------
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 3.5 * K
# SEQ_LEN = 4 * K
# LAYER_NUM = 32
# NUM_ATTENTION_HEAD=40
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 2
# # #Qwen 7(8)B I1F1B --------------
CHUNK_NUM = LAYER_NUM // DEVICE_NUM
# CHUNK_NUM = 2
