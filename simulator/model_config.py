
G = 1024 * 1024 * 1024
M = 1024 * 1024
K = 1024
B = G
# DEVICE_NUM = 4 * 2
# # 自定义模型
# VOCAB_SIZE = 92544
# NUM_ATTENTION_HEAD = 32
# SEQ_LEN = 4 * K
# HIDDEN_SIZE = 4 * K
# LAYER_NUM = 4 * 4 * 1
MICRO_BATCH_SIZE = 1
# MICRO_BATCH_NUM = 4 * 4 * 1

# WORLD_SIZE = DEVICE_NUM
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 1
# MODEL_TYPE = "GPT"

# Qwen 72B I1F1B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 8
# VOCAB_SIZE = 128256
# MLP_RATIO = 2.6875
# HIDDEN_SIZE = 8 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 80
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# Qwen 72B I1F1B --------------

# Llama2 56B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 8
# VOCAB_SIZE = 128256
# MLP_RATIO = 2.6875
# HIDDEN_SIZE = 8 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 64
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# Llama 56B --------------


# Llama2 14B --------------
MODEL_TYPE = "LLAMA"
DEVICE_NUM = 8
VOCAB_SIZE = 128256
MLP_RATIO = 2.6875
HIDDEN_SIZE = 8 * K
INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
SEQ_LEN = 4 * K
LAYER_NUM = 4
NUM_ATTENTION_HEAD=64
PP_SIZE = DEVICE_NUM
TP_SIZE = 1
ZERO_SIZE = 2
MICRO_BATCH_NUM = 8
# Llama 14B --------------

# Llama2 42B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 4
# VOCAB_SIZE = 128256
# MLP_RATIO = 2.6875
# HIDDEN_SIZE = 8 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 16
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 8
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# Llama 42B --------------


# gpt 175B --------------
# MODEL_TYPE = "GPT"
# DEVICE_NUM = 8
# VOCAB_SIZE = 30000
# MLP_RATIO = 4
# HIDDEN_SIZE = 12 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 96
# NUM_ATTENTION_HEAD=96
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 8
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# gpt 175B --------------

# # Llama3 405B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 16
# VOCAB_SIZE = 128256
# HIDDEN_SIZE = 16 * K
# MLP_RATIO = 53248 / HIDDEN_SIZE
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 128
# NUM_ATTENTION_HEAD=128
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 8
# ZERO_SIZE = 4
# MICRO_BATCH_NUM = PP_SIZE * 4
# # Llama 405B --------------

# Qwen 72B --------------
# MODEL_TYPE = "Qwen"
# DEVICE_NUM = 8
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 8 * K
# MLP_RATIO = 29568 / HIDDEN_SIZE
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 80
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = 4*DEVICE_NUM
# Qwen 72B --------------

# #Qwen 32B I1F1B --------------
# MODEL_TYPE = "Qwen"
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 5 * K
# MLP_RATIO = 27648 / HIDDEN_SIZE
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 64
# NUM_ATTENTION_HEAD=40
# DEVICE_NUM = 4
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 1
# ZERO_SIZE = 4
# MICRO_BATCH_NUM = 8
# #Qwen 32B I1F1B --------------

# #Qwen 32B I1F1B --------------
# MODEL_TYPE = "Qwen"
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 5 * K
# MLP_RATIO = 27648 / HIDDEN_SIZE
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 32
# NUM_ATTENTION_HEAD=40
# DEVICE_NUM = 4
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 1
# ZERO_SIZE = 4
# MICRO_BATCH_NUM = 8
# #Qwen 32B I1F1B --------------

# # #Qwen 14B I1F1B --------------
# MODEL_TYPE = "Qwen"
# DEVICE_NUM = 4
# MLP_RATIO = 5.25
# VOCAB_SIZE = 151936
# HIDDEN_SIZE = 5 * K
# SEQ_LEN = 2 * K
# LAYER_NUM = 48
# NUM_ATTENTION_HEAD=40
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = 12#PP_SIZE*2
# INTER_SIZE = 17408
# # #Qwen 14B I1F1B --------------

# # #Qwen 7(8)B I1F1B --------------
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 3.5 * K
# SEQ_LEN = 4 * K
# LAYER_NUM = 32
# NUM_ATTENTION_HEAD=40
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 2

# MODEL_TYPE = "Qwen"
# DEVICE_NUM = 4
# WORLD_SIZE = DEVICE_NUM
# MICRO_BATCH_NUM = 16
# VOCAB_SIZE = 152064
# SEQ_LEN = 2048
# HIDDEN_SIZE = 3584
# NUM_ATTENTION_HEAD = 28
# NUM_KV_ATTENTION_HEAD = 4
# MLP_RATIO = 5.25
# LAYER_NUM = 32
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 1
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# # #Qwen 7(8)B I1F1B --------------

#INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
CHUNK_NUM = LAYER_NUM // DEVICE_NUM
# CHUNK_NUM = 2
