
G = 1024 * 1024 * 1024
M = 1024 * 1024
K = 1024
B = G
DEVICE_NUM = 4 * 2
# 自定义模型
VOCAB_SIZE = 92544
NUM_ATTENTION_HEAD = 32
SEQ_LEN = 4 * K
HIDDEN_SIZE = 4 * K
LAYER_NUM = 4 * 4 * 1
MICRO_BATCH_SIZE = 1
MICRO_BATCH_NUM = 4 * 4 * 1

WORLD_SIZE = DEVICE_NUM
PP_SIZE = DEVICE_NUM
TP_SIZE = 1
MODEL_TYPE = "GPT"

# Qwen 72B I1F1B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 8
# VOCAB_SIZE = 128256
# MLP_RATIO = 2.6875
# HIDDEN_SIZE = 8 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 80
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# Qwen 72B I1F1B --------------

# Llama2 56B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 8
# VOCAB_SIZE = 128256
# MLP_RATIO = 2.6875
# HIDDEN_SIZE = 8 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 64
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 4
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# Llama 56B --------------


# Llama2 14B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 8
# VOCAB_SIZE = 128256
# MLP_RATIO = 2.6875
# HIDDEN_SIZE = 8 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 16
# NUM_ATTENTION_HEAD=64
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 1
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# Llama 14B --------------

# Llama2 42B --------------
MODEL_TYPE = "LLAMA"
DEVICE_NUM = 4
VOCAB_SIZE = 128256
MLP_RATIO = 2.6875
HIDDEN_SIZE = 8 * K
INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
SEQ_LEN = 4 * K
LAYER_NUM = 16
NUM_ATTENTION_HEAD=64
PP_SIZE = DEVICE_NUM
TP_SIZE = 8
ZERO_SIZE = 1
MICRO_BATCH_NUM = PP_SIZE * 4
# Llama 42B --------------


# gpt 175B --------------
# MODEL_TYPE = "GPT"
# DEVICE_NUM = 8
# VOCAB_SIZE = 30000
# MLP_RATIO = 4
# HIDDEN_SIZE = 12 * K
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 96
# NUM_ATTENTION_HEAD=96
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 8
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# gpt 175B --------------

# # Llama3 405B --------------
# MODEL_TYPE = "LLAMA"
# DEVICE_NUM = 16
# VOCAB_SIZE = 128256
# HIDDEN_SIZE = 16 * K
# MLP_RATIO = 53248 / HIDDEN_SIZE
# INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
# SEQ_LEN = 4 * K
# LAYER_NUM = 128
# NUM_ATTENTION_HEAD=128
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 8
# ZERO_SIZE = 4
# MICRO_BATCH_NUM = PP_SIZE * 4
# # Llama 405B --------------

# Qwen 72B --------------
MODEL_TYPE = "Qwen"
DEVICE_NUM = 8
VOCAB_SIZE = 152064
HIDDEN_SIZE = 8 * K
MLP_RATIO = 29568 / HIDDEN_SIZE
INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
SEQ_LEN = 4 * K
LAYER_NUM = 80
NUM_ATTENTION_HEAD=64
PP_SIZE = DEVICE_NUM
TP_SIZE = 1
ZERO_SIZE = 2
MICRO_BATCH_NUM = 16
# Qwen 72B --------------

# #Qwen 32B I1F1B --------------
MODEL_TYPE = "Qwen"
VOCAB_SIZE = 152064
HIDDEN_SIZE = 5 * K
MLP_RATIO = 27648 / HIDDEN_SIZE
INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
SEQ_LEN = 4 * K
LAYER_NUM = 64
NUM_ATTENTION_HEAD=40
DEVICE_NUM = 4
PP_SIZE = DEVICE_NUM
TP_SIZE = 1
ZERO_SIZE = 4
MICRO_BATCH_NUM = 8
# #Qwen 32B I1F1B --------------

# #Qwen 32B I1F1B --------------
MODEL_TYPE = "Qwen"
VOCAB_SIZE = 152064
HIDDEN_SIZE = 5 * K
MLP_RATIO = 27648 / HIDDEN_SIZE
INTER_SIZE = HIDDEN_SIZE * MLP_RATIO
SEQ_LEN = 4 * K
LAYER_NUM = 16
NUM_ATTENTION_HEAD=40
DEVICE_NUM = 4
PP_SIZE = DEVICE_NUM
TP_SIZE = 1
ZERO_SIZE = 4
MICRO_BATCH_NUM = 16
# #Qwen 32B I1F1B --------------

# # #Qwen 14B I1F1B --------------
# DEVICE_NUM = 8
# VOCAB_SIZE = 152064
# HIDDEN_SIZE = 5 * K
# SEQ_LEN = 4 * K
# LAYER_NUM = 48
# NUM_ATTENTION_HEAD=40
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 1
# ZERO_SIZE = 2
# MICRO_BATCH_NUM = PP_SIZE * 4
# # #Qwen 14B I1F1B --------------

# Gemma
G_SCALE = 2
VOCAB_SIZE = 256 * 1024 * G_SCALE
DEVICE_NUM = 4 * 2
HIDDEN_SIZE = 1.5 * K
SEQ_LEN = 4 * K
LAYER_NUM = 32 * G_SCALE
NUM_ATTENTION_HEAD=32
PP_SIZE = DEVICE_NUM
TP_SIZE = 4
ZERO_SIZE = 1
MICRO_BATCH_NUM = PP_SIZE * 2
CHUNK_NUM = LAYER_NUM // DEVICE_NUM
CHUNK_NUM = 1

# # DeepSeek
# DS_SCALE = 4
# DEEPSEEK_VOC = 128 * 1024
# VOCAB_SIZE = 128 * 1024 * DS_SCALE
# DEVICE_NUM = min(4 * DS_SCALE, 8)
# HIDDEN_SIZE = 2 * K
# SEQ_LEN = 4 * K
# LAYER_NUM = 16 * DS_SCALE
# NUM_ATTENTION_HEAD=16
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 2
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# # MICRO_BATCH_NUM = 4
# # # #Qwen 7(8)B I1F1B --------------
# CHUNK_NUM = LAYER_NUM // DEVICE_NUM
# CHUNK_NUM = 1

# NemotronH
N_SCALE = 4
# VOCAB_SIZE = 128 * 1024 * N_SCALE
# DEVICE_NUM = 4 * N_SCALE
# DEVICE_NUM = 4 * 4
# HIDDEN_SIZE = 1.5 * K
# SEQ_LEN = 2 * K * 1
# LAYER_NUM = 28 * N_SCALE
# NUM_ATTENTION_HEAD=32
# PP_SIZE = DEVICE_NUM
# TP_SIZE = 2
# ZERO_SIZE = 1
# MICRO_BATCH_NUM = PP_SIZE * 4
# CHUNK_NUM = LAYER_NUM // DEVICE_NUM
# CHUNK_NUM = 1